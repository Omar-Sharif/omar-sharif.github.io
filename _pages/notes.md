---
title: "My Notes"
excerpt: ""
author_profile: true
permalink: /notes/
redirect_from: 
  - /about.html
---
My hand notes while I completed the <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a> on Coursera.
 * Neural Networks and Deep Learning <a href="https://drive.google.com/file/d/1P6iF2SFhkT9jWwTU431mN644HGYcQdpu/view?usp=sharing">Note</a>.
 * Improving Deep Neural Networks  <a href="https://drive.google.com/file/d/1P6iF2SFhkT9jWwTU431mN644HGYcQdpu/view?usp=sharing">Note</a>.
 * Structuring Machine Learning Projects <a href="https://drive.google.com/file/d/1P6iF2SFhkT9jWwTU431mN644HGYcQdpu/view?usp=sharing">Note</a>.
 * Convolutional Neural Networks <a href="https://drive.google.com/file/d/1bCqlppOAW270Q4ZEv3lI6uw0-Zs1BI23/view?usp=sharing">Note</a>.
 * Sequence Models <a href="https://drive.google.com/file/d/14o0ptgBZw8sdzFXg35NqEL5Ar8KrN3_L/view?usp=sharing">Note</a>.

## <font color="#00cc66"> Classical NLP Paper </font>
I found these papers useful in clarifying my understanding of various NLP topics. 
<pre>
* <span style="color:rgb(201, 76, 76)"><a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a> </span> 
* <span style="color:rgb(201, 76, 76)"><a href="https://arxiv.org/abs/1301.3781">[Word2Vec]</a> <a href="https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html">[Negative Sampling]</a> <a href="https://aclanthology.org/D14-1162.pdf">[GloVe]</a> </span>
* <span style="color:rgb(201, 76, 76)"><a href="https://aclanthology.org/D14-1179.pdf">Learning Phrase Representations using RNN Encoder–Decoder
for Statistical Machine Translation</a> </span>
* <span style="color:rgb(201, 76, 76)"><a href="https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf">Sequence to Sequence Learning with Neural Networks</a> </span>
* <span style="color:rgb(201, 76, 76)"><a href="https://arxiv.org/pdf/1409.0473.pdf">Neural Machine Translation by Jointly Learning to Align and Translate (Paper that introduces Attention) </a> </span>
* <span style="color:rgb(201, 76, 76)"><a href="https://aclanthology.org/D14-1179.pdf">Learning Phrase Representations using RNN Encoder–Decoder
for Statistical Machine Translation</a> </span>

</pre>


## <font color="#00cc66"> Useful Links </font>
Few blog posts/links that I found really useful to understand various fundamental concepts of NLP.
<pre>
* <span style="color:rgb(201, 76, 76)">Andrej Karpathy's coding-based backpropagation post</span> <a href="http://karpathy.github.io/neuralnets/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">Andrej Karpathy's blog on RNNs</span> <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">Understanding LSTM Networks</span> <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">The Illustrated Word2vec</span> <a href="https://jalammar.github.io/illustrated-word2vec/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">Mechanics of Seq2seq Models With Attention</span> <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">The Illustrated Transformer</span> <a href="https://jalammar.github.io/illustrated-transformer/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">The Annotated Transformer</span> <a href="http://nlp.seas.harvard.edu/annotated-transformer/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">Visualizing Transformer Language Models</span> <a href="https://jalammar.github.io/illustrated-gpt2/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">The State of Transfer Learning in NLP</span> <a href="https://www.ruder.io/state-of-transfer-learning-in-nlp/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">The Illustrated BERT, ELMo, and co.</span> <a href="https://jalammar.github.io/illustrated-bert/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">A Visual Guide to Using BERT</span> <a href="https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/">[Link]</a>
* <span style="color:rgb(201, 76, 76)">Various BERT Pre-Training Methods</span> <a href="https://medium.com/analytics-vidhya/an-overview-of-the-various-bert-pre-training-methods-c365512342d8">[Link]</a>
</pre>

  
